# 1. Welcome 

Welcome to Introduction to Big Data! We’ve been working with learners, like yourself, to design a course for beginners to get familiar with the potential of Big Data -- while also giving those with some background the opportunity to think about how Big Data may stand to impact their lives and businesses.

Here’s an overview of our goals for you in the course. After completing this course you should be able to:

- Describe the Big Data landscape including examples of real world big data problems including the three key sources of Big Data: people, organizations, and sensors. 

- Explain the V’s of Big Data (volume, velocity, variety, veracity, valence, and value) and why each impacts data collection, monitoring, storage, analysis and reporting.

- Get value out of Big Data by using a 5-step process to structure your analysis. 

- Identify what are and what are not big data problems and be able to recast big data problems as data science questions. 

- Provide an explanation of the architectural components and programming models used for scalable big data analysis. 

- Summarize the features and value of core Hadoop stack components including the YARN resource and job management system, the HDFS file system and the MapReduce programming model. 

- Install and run a program using Hadoop!

Throughout the course, we offer you various ways to engage and test your proficiency with these goals. Required quizzes seek to give you the opportunity to retrieve core definitions, terms, and key take-away points. We know from research that the first step in gaining proficiency with something involves repeated practice to solidify long-term memory. 

But, we also offer a number of optional discussion prompts where we encourage you to think about the concepts covered as they might impact your life or business. We encourage you to both contribute to these discussions and to read and respond to the posts of others. This opportunity to consider the application of new concepts to problems in your own life really helps deepen your understanding and ability to utilize the new knowledge you have learned. 

Finally, we know this is an introductory course, but we offer you one problem solving opportunity to give you practice in applying the Map Reduce process. Map Reduce is a core programming model for Big Data analysis and there’s no better way to make sure you really understand it than by trying it out for yourself!

We hope that you will find this course both accessible, but also capable of helping you deepen your thinking about the core concepts of Big Data. Remember, this is just the start to our specialization -- but it’s also a great time to take a step back and think about why the challenges of Big Data now exist and how you might see them impacting your world -- or the world in the future!

Welcome!
Ilkay Altintas and Amarnath Gupta
San Diego Supercomputer Center
University of California, San Diego

## By the end of this course you will be able to...

Describe the Big Data landscape including examples of real world big data problems and approaches.
Identify the high level components in the data science lifecycle and associated data flow.
Explain the V’s of Big Data and why each impacts the collection, monitoring, storage, analysis and reporting, including their impact in the presence of multiple V’s.
Identify big data problems and be able to recast problems as data science questions.
Summarize the features and significance of the HDFS file system and the MapReduce programming model and how they relate to working with Big Data.

## Have you ever wondered what a supercomputer is?

At the following video, learn about what supercomputers are, what kinds of people work at a supercomputer center, and some fun facts about San Diego Supercomputer Center, where the instructors of this specialization work.

https://www.youtube.com/watch?v=x87GrXgmQ2k